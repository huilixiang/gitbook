### 绪论
- 背景： 寻找数据中的模式
- 泛化： 正确分类与训练集不同的新样本的能力叫泛化， 由于训练集很可能只是所有输入的一很小， 部分是模型识别中的一个核心问题
- 特征抽取： 原始向量被预处理，变换到新的变量空间，以期望在新的变量空间中模型识别的问题可以被更容易的解决
- 强化学习： 要解决的问题：在给定的条件下，找到合适的动作使得奖励达到最大化。 在这里学习问题没有给定最优输出的用例。这些用例必须是在一系列的实验和错误中被发现的。
- 多项式拟合: y是关于w的线性函数。关于未知参数满足线性关系的函数有着重要的性质，被叫做线性模型。
- 概率论： 对不确定性进行量化和计算
- 概率密度： 如果一个实值变量x落在区间(x, $$x + \delta x$$)的概率由p(x)$$\delta x$$给出， 那么p(x)叫做x的概率密度函数
- 实值变量x及其pdf: $$p_x(x)$$, 如果存在一个非线性变换：x = g(y), 且变量y及其pdf为： $$p_y(y)$$,  <br/>  
  则函数$$f(x) = f(g(y))$$ . 关于很小的$$\delta x$$, 落在(x, x+$$\delta x$$)区间的观测会被变换到区间(y, $$y + \delta y$$)内。其中$$p_x(x)\delta x \simeq p_y(y) \delta y$$   因此<br/> 
  $$p_y(y) = p_x(x) \left | \frac{dx}{dy}\right | = p_x(g(y)) \left | g'(y)\right |$$
- 概率的乘积、加和规则以及贝叶斯规则适用于pdf

#### 期望与协方差
- 期望： 加权平均值 $$E[f] = \sum_x p(x)f(x)$$
- 方差:  $$var[f] = E[(f(x) - E[f(x)])^2]$$, 度量f(x) 在均值附近变化性的大小， 也可以写成：$$E[f(x)^2] - E[f(x)]^2$$
- 协方差:  $$cov[x,y] = E[(x - E[x])(y - E[y])] = E_{x,y}[xy] - E_x[x]E_y[y] $$ 它度量的是两个变量有多大程度会共同变化<br/>

#### 贝叶斯概率
- 经典概率： 随机重复事件的频率来表示概率
- 贝叶斯： 无法重复的事件，希望能够定量的描述不确定性，并希望通过通过少量的证据对不确定性进行精确修正。
- 似然函数： 数据集D及未知参数w, 则贝叶斯公式： <br/>
  $$p(w|D) = \frac{p(D|w)p(w)}{p(D)}$$ <br/> 
  能让我们通过后验概率p(w|D), 在观测到D之后估计w的不确定性。 右侧的p(D|w)由观测数据集来估计，可以被看成是关于参数向量w的函数，被称为似然函数。它表达了在不同的参数向量w下，观测数据出现的可能性大小。注意似然函数不是w的概率分布，并且它关于w的积分不一定等于1. <br/>
  $$posterior \propto likelihood \times prior $$ 
- 似然函数在贝叶斯派和频率派的区别
    1. 频率学派中， w是一个固定的未知参数， 通过某种形式的“估计”来确定，这个估计的误差通过考察可能的数据集D的概率分布得到。 如极大似然估计，选择使似然函数达到最大的w值，这对应对选择例观察到的数据出现概率最大的w值。似然函数的负对数是误差函数，由于负对数是单调递减函数函数， 极大似然等于最小误差。 可以使用bootstrap来决定误差
    2. 贝叶斯派， 只有一个数据集D， 参数的不确定性通过w的概率分布来表达。 先验是为了计算方便
  

#### 似然函数与概率密度函数
- 

#### 贝叶斯曲线拟合
- 贝叶斯方法始终仅使用概率的乘积和加和规则，后验概率是个分布，而不是个具体的值。故一般会先用后验分布的众数，均值 ， 中位数等进行点估计
- 贝叶斯曲线拟合。现有数据集X，标签T，以参数w, 以及一个新的测试点x, 预期其对应的t, 则预测概率可以写成如下形式： <br/>
$$p(t|x, X, w) = \int p(t|x,w) p(w|X,T) dw$$ 其结果是一个概率分布。

#### 模型选择
- 信息准则： 各种信息准则主要用来修正最大似然的偏差
#### 维度灾难
- 我们在三维空间中建立的几何直觉在高维空间中失效。 考虑D维空间的一个半径为r的球， 球体的大部分体积都聚集在表面附近的薄壳上。
#### 决策论
- 决策论与概率论相结合，我们能在涉及不确定性的时候做出最优决策
#### 推断和决策
- 推断：使用训练数据学习 p(c|x), 在决策阶段使用后验概率来进行最优的分类。
- 判别模型： 另外一种可能的方法：直接学习一个函数，将输入x映射为决策，这类的函数称为差别函数。

##### 三种不同的方法（难度递减）
1. 生成模型： 直接对联合概率p(c, x)建模，然后归一化，求得后验概率。通过决策论来确定每个新的输入x的类别。
2. 判别模型： 推断后验概率p(c|x), 使用决策论来对新的输入x进行分类
3. 找到一个差别函数， 直接将x遇到c。如二分类，此时概率不起作用

##### 生成模型的优劣点
- 劣势： 生成模型需要求解的东西最多，涉及寻找x与c的联合分布，在高维情况下，需要大量的数据才能在合理的精度下确定类条件概率。 p(c)一般可以通过训练数据中各类别的比例简单的估计出来。   
- 优势： 求得了p(x), 对于检测模型中低概率的新数据点有用， 离群点、异常点检测。
##### 直接进行分类决策的优劣点：
- 劣势： 不能够接触后验概率，很多事情不能够进行：
    1. 最小化风险。 损失矩阵不能时刻更新，最小化风险准则计算不方便
    2. 拒绝选项。 无法进行估计
    3. 补偿先验概率。当正负比例严重失衡时。。我们会构建一个样本比例相对均衡的训练集，如果我们使用后验概率就可以进行先验补偿： 后难概率除以当前训练集的类比例然后再乘以目标人群中的类比例即可。 如果直接学习一个判别函数，则此步骤不能进行
    4. 组合模型： 利用贝叶斯法则进行模型组合。 假设类条件概率独立，那么如果有两个子系统X , Y, 则： <br/>
    
    


