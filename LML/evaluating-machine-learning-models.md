## machine learning jargon
- deep learning
- the kernel trick
- regularization
- overfitting
- semi-supervised learning
- cross-validation
## outset question
- How can I measure success for this project?
- How would I know when Iâ€™ve succeeded?

## machine learning workflow
- prototype---model selection  
- deployed model
- offline & online evaluation

## why is it so complicated?
### difference between off/online evaluation
- offline  </br>
    >accuracy ; precision-recall
- online <br/>
    >business metrics: LTV
    
### distribution drift
- historical and live data
- assume:  the distribution of data stays the same over time

## evaluation metrics
### classification metrics
- accuracy <br/>
    > measures how often the classifier makes the correct prediction.
    > correct answers for class 0 and class 1 are treated equally
- confusion matrix
- log-loss
- auc
- 

### offline evaluation mechanisms
- prototype: select right model to fit the data
- hold-out validation, also as known: k-fold cross-validation



