# 特征工程及模型

##模型评估
###现状问题
用钱宝评测细则虽然能够从宏观指标上评估新模型的有效性和准确性， 但缺失很多细节， 当新模型上线后往往难以达到预期且新模型在线上表现不佳时，很难给出相应的解释，在此整理之前的评估标准缺失的环节

###样本
- 样本容量怎样确定？     例: 当做下探实验或者新产品试探的时候，模型需要的最小样本量是多少？
- 如何评估样本分布的一致性？   例：当阈值向上调整或外部市场环境突变时间段内的样本可直接与原样本混用么？
- 怎样选择测试集和训练集？       例：模型的训练集和测试集如何划分？需要与线上的应用场景一致么？
 

###特征
- 特征的相关性和交互作用是怎样的？   例：特征有相关性么？特征a是不是在特征b存在的时候效果才有提升？300+维的特征auc 0.7, 某单特征auc 0.62, 堆特征真的合理么？
- 特征的重要性如何评估？                     例：当前模型的重要特征有哪些？ 与对比模型的重要特征有变化么？
- 特征的重要性与时间序列的关系是什么？  例： 时效性更新会导致特征的重要性发生变化么？也即：随着时间的变化，特征的语义发生了反转？
- 如何确定特征的异常、缺失值？                 例： 特征值的分布如果不是正态的，怎么确定异常值？ 缺失值真的可以直接填充0么？如果存在有效特征值0呢？
- 特征的异常、缺失值对模型效果的影响如何评估？ 例：异常、缺失处理对模型的影响怎样评估？
- 不同特征变换的适用场景是什么？      例：做算法模型对比时，所有的算法都需要分桶么？ 或者分桶算法需要一致么？
- 特征对模型效果的影响如何量化？      例：特征值的变化会与模型效果成某种关系么？

###算法
- 算法模型的假设是否满足？             例： 特征与模型是线性/非线性关系么？
- 模型在训练集/测试集的损失是怎样的？ 例： 同样auc时，哪个模型更好呢？
- 模型的最佳分类阈值是什么样的？    例： 模型在哪个分段分类效果最好?
- 分档的准、召率是怎样的？        例： 如果分档的准、召率是怎样的？ 通过阈值可以直接下探么？还是看实际情况，人工调整？
- 分档置信度和置信区间是怎样的？ 例： 模型阈值下探的置信度和区间分别是多少？只能一个月后看实际表现么？

### 评估指标
#### 样本
- 最小样本容量
- 样本分布统计量
- 训练集、测试集样本分布统计量
#### 特征
- 单特征的重要度指标
- 特征的相关性指标
- 特征间的交互作用指标
- 特征值分布统计量
- 特征异常值、缺失值统计量
- 特征权重的置信度及置信区间
#### 算法
- 算法假设
- 训练集、测试集损失
- 模型的最佳分类阈值
- 分档准、召率
- 分档预估值的置信度及置信区间

### 拆解细分

- 模型上线仍然分： 样本工程、特征工程、模型工程。 
- 但评估流程目前应该一致： 除了原有的一些统计量外， 必须要有样本、特征、模型相关的细节指标。 

- 以时效性更新为例： 加入新样本的目的是想引入特征最近的分类能力， 但如果在最近时间段内特征的含义与之前发生了反转，那模型学到了什么？ 如果特征的含义不发生变化，那更新的意义又何在？仅是特征语义强化 了？那为什么每次时效性更新auc几乎不变？